---
layout: default
title: Publications -- Michael I Mandel
---
<div class="page-header">
<h1>Publications</h1>
</div>
<h2>Theses, Chapters</h2>
<table>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="mandel16e">1</a>]
</td>
<td class="bibtexitem">
Michael&nbsp;I Mandel, Shoko Araki, and Tomohiro Nakatani.
 Multichannel clustering and classification approaches.
 In Emmanuel Vincent, Tuomas Virtanen, and Sharon Gannot, editors,
  <em>Audio Source Separation and Speech Enhancement</em>, chapter&nbsp;12. Wiley,
  2018.
 To appear.
[&nbsp;<a href="pubs_bib.html#mandel16e">bib</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="MandelAndBarker2017">2</a>]
</td>
<td class="bibtexitem">
Michael&nbsp;I Mandel and Jon&nbsp;P Barker.
 Multichannel spatial clustering using model-based source separation.
 In Shinji Watanabe, Marc Delcroix, Florian Metze, and John&nbsp;R.
  Hershey, editors, <em>New Era for Robust Speech Recognition: Exploiting,
  Deep Learning</em>, chapter&nbsp;3. Springer, 2017.
[&nbsp;<a href="pubs_bib.html#MandelAndBarker2017">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/978-3-319-64680-0">DOI</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="XiaoEtAl2017">3</a>]
</td>
<td class="bibtexitem">
Xiong Xiao, Shinji Watanabe, Hakan Erdogan, Michael Mandel, Liang Lu, John&nbsp;R.
  Hershey, Michael&nbsp;L. Seltzer, Guoguo Chen, Yu&nbsp;Zhang, and Dong Yu.
 Discriminative beamforming with phase-aware neural networks for
  speech enhancement and recognition.
 In Shinji Watanabe, Marc Delcroix, Florian Metze, and John&nbsp;R.
  Hershey, editors, <em>New Era for Robust Speech Recognition: Exploiting,
  Deep Learning</em>, chapter&nbsp;4. Springer, 2017.
[&nbsp;<a href="pubs_bib.html#XiaoEtAl2017">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/978-3-319-64680-0">DOI</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="devaney16b">4</a>]
</td>
<td class="bibtexitem">
Johanna Devaney, Michael&nbsp;I Mandel, Douglas Turnbull, and George Tzanetakis,
  editors.
 <em>Proceedings of the 17th International Society for Music
  Information Retrieval Conference (ISMIR)</em>.
 New York, 2016.
[&nbsp;<a href="pubs_bib.html#devaney16b">bib</a>&nbsp;| 
<a href="https://drive.google.com/file/d/0B2SQvWn0_78BaWxUNEdyakROLWM/view?usp=sharing">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="bertin-mahieux09">5</a>]
</td>
<td class="bibtexitem">
Thierry Bertin-Mahieux, Douglas Eck, and Michael&nbsp;I. Mandel.
 Automatic tagging of audio: The state-of-the-art.
 In Wenwu Wang, editor, <em>Machine Audition: Principles, Algorithms
  and Systems</em>, chapter&nbsp;14, pages 334--352. IGI Publishing, 2010.
[&nbsp;<a href="pubs_bib.html#bertin-mahieux09">bib</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="mandel09c">6</a>]
</td>
<td class="bibtexitem">
Michael&nbsp;I. Mandel.
 <em>Binaural Model-Based Source Separation and Localization</em>.
 PhD thesis, Columbia University, February 2010.
[&nbsp;<a href="pubs_bib.html#mandel09c">bib</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/dissertation.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">
When listening in noisy and reverberant environments, human listeners are able to focus on a particular sound of interest while ignoring interfering sounds. Computer listeners, however, can only perform highly constrained versions of this task. While automatic speech recognition systems and hearing aids work well in quiet conditions, source separation is necessary for them to be able to function in these challenging situations.<p>
This dissertation introduces a system that separates more than two sound sources from reverberant, binaural mixtures based on the sources' locations. Each source is modelled probabilistically using information about its interaural time and level differences at every frequency, with parameters learned using an expectation maximization (EM) algorithm. The system is therefore called Model-based EM Source Separation and Localization (MESSL). This EM algorithm alternates between refining its estimates of the model parameters (location) for each source and refining its estimates of the regions of the spectrogram dominated by each source. In addition to successfully separating sources, the algorithm estimates model parameters from a mixture that have direct psychoacoustic relevance and can usually only be measured for isolated sources. One of the key features enabling this separation is a novel probabilistic localization model that can be evaluated at individual time-frequency points and over arbitrarily-shaped regions of the spectrogram.<p>
The localization performance of the systems introduced here is comparable to that of humans in both anechoic and reverberant conditions, with a 40% lower mean absolute error than four comparable algorithms. When target and masker sources are mixed at similar levels, MESSL's separations have signal-to-distortion ratios 2.0 dB higher than four comparable separation algorithms and estimated speech quality 0.19 mean opinion score units higher. When target and masker sources are mixed anechoically at very different levels, MESSL's performance is comparable to humans', but in similar reverberant mixtures it only achieves 20–-25% of human performance. While MESSL successfully rejects enough of the direct-path portion of the masking source in reverberant mixtures to improve energy-based signal-to-noise ratio results, it has difficulty rejecting enough reverberation to improve automatic speech recognition results significantly. This problem is shared by other comparable separation systems.
</font></blockquote>
<p>
</td>
</tr>
</table>

<h2>Journal</h2>
<table>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="mandel16c">1</a>]
</td>
<td class="bibtexitem">
Michael&nbsp;I Mandel, Sarah&nbsp;E Yoho, and Eric&nbsp;W Healy.
 Measuring time-frequency importance functions of speech with bubble
  noise.
 <em>Journal of the Acoustical Society of America</em>, 140:2542--2553,
  2016.
[&nbsp;<a href="pubs_bib.html#mandel16c">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1121/1.4964102">DOI</a>&nbsp;| 
<a href="http://github.com/mim/auditoryBubbles">Code</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/jasa16.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">
Listeners can reliably perceive speech in noisy conditions, but it is not well understood what specific features of speech they use to do this. This paper introduces a data-driven framework to identify the time-frequency locations of these features. Using the same speech utterance mixed with many different noise instances, the framework is able to compute the importance of each time-frequency point in the utterance to its intelligibility. The mixtures have approximately the same global signal-to-noise ratio at each frequency, but very different recognition rates. The difference between these intelligible vs unintelligible mixtures is the alignment between the speech and spectro-temporally modulated noise, providing different combinations of “glimpses” of speech in each mixture. The current results reveal the locations of these important noise-robust phonetic features in a restricted set of syllables. Classification models trained to predict whether individual mixtures are intelligible based on the location of these glimpses can generalize to new conditions, successfully predicting the intelligibility of novel mixtures. They are able to generalize to novel noise instances, novel productions of the same word by the same talker, novel utterances of the same word spoken by different talkers, and, to some extent, novel consonants.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="larochelle12">2</a>]
</td>
<td class="bibtexitem">
Hugo Larochelle, Michael&nbsp;I Mandel, Razvan Pascanu, and Yoshua Bengio.
 Learning algorithms for the classification restricted boltzmann
  machine.
 <em>Journal of Machine Learning Research</em>, 13:643--669, March 2012.
[&nbsp;<a href="pubs_bib.html#larochelle12">bib</a>&nbsp;| 
<a href="http://www.jmlr.org/papers/volume13/larochelle12a/larochelle12a.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">
Recent developments have demonstrated the capacity of restricted
Boltzmann machines (RBM) to be powerful generative models, able to
extract useful features from input data or construct deep artificial
neural networks. In such settings, the RBM only yields a preprocessing
or an initialization for some other model, instead of acting as a
complete supervised model in its own right. In this paper, we argue
that RBMs can provide a self-contained framework for developing
competitive classifiers. We study the Classification RBM (ClassRBM), a
variant on the RBM adapted to the classification setting. We study
different strategies for training the ClassRBM and show that
competitive classification performances can be reached when
appropriately combining discriminative and generative training
objectives. Since training according to the generative objective
requires the computation of a generally intractable gradient, we also
compare different approaches to estimating this gradient and address
the issue of obtaining such a gradient for problems with very high
dimensional inputs. Finally, we describe how to adapt the ClassRBM to
two special cases of classification problems, namely semi-supervised
and multitask learning.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="weiss11">3</a>]
</td>
<td class="bibtexitem">
Ron Weiss, Michael&nbsp;I. Mandel, and Daniel P.&nbsp;W. Ellis.
 Combining localization cues and source model constraints for binaural
  source separation.
 <em>Speech Communication</em>, 53(5):606--621, May 2011.
[&nbsp;<a href="pubs_bib.html#weiss11">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1016/j.specom.2011.01.003">DOI</a>&nbsp;| 
<a href="http://www.ee.columbia.edu/~dpwe/pubs/WeissME11-messlev.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">
We describe a system for separating multiple sources from a two-channel recording based on interaural cues and prior knowledge of the statistics of the underlying source signals. The proposed algorithm effectively combines information derived from low level perceptual cues, similar to those used by the human auditory system, with higher level information related to speaker identity. We combine a probabilistic model of the observed interaural level and phase differences with a prior model of the source statistics and derive an EM algorithm for finding the maximum likelihood parameters of the joint model. The system is able to separate more sound sources than there are observed channels in the presence of reverberation. In simulated mixtures of speech from two and three speakers the proposed algorithm gives a signal-to-noise ratio improvement of 1.7 dB over a baseline algorithm which uses only interaural cues. Further improvement is obtained by incorporating eigenvoice speaker adaptation to enable the source model to better match the sources present in the signal. This improves performance over the baseline by 2.7 dB when the speakers used for training and testing are matched. However, the improvement is minimal when the test data is very different from that used in training.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="mandel11b">4</a>]
</td>
<td class="bibtexitem">
Michael&nbsp;I. Mandel, Razvan Pascanu, Douglas Eck, Yoshua Bengio, Luca&nbsp;M. Aiello,
  Rossano Schifanella, and Filippo Menczer.
 Contextual tag inference.
 <em>ACM Transactions on Multimedia Computing, Communications and
  Applications</em>, 7S(1):32:1--32:18, October 2011.
[&nbsp;<a href="pubs_bib.html#mandel11b">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1145/2037676.2037689">DOI</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/tomccap11.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">
This paper examines the use of two kinds of context to improve the results of content-based music taggers: the relationships between tags and between the clips of songs that are tagged.  We show that users agree more on tags applied to clips temporally "closer" to one another; that conditional restricted Boltzmann machine models of tags can more accurately predict related tags when they take context into account; and that when training data is "smoothed" using context, support vector machines can better rank these clips according to the original, unsmoothed tags and do this more accurately than three standard multi-label classifiers.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="devaney12a">5</a>]
</td>
<td class="bibtexitem">
Johanna Devaney, Michael&nbsp;I. Mandel, Daniel P.&nbsp;W. Ellis, and Ichiro Fujinaga.
 Automatically extracting performance data from recordings of trained
  singers.
 <em>Psychomusicology: Music, Mind &amp; Brain</em>, 21(1–-2):108--136,
  2012.
[&nbsp;<a href="pubs_bib.html#devaney12a">bib</a>&nbsp;| 
<a href="http://music.mcgill.ca/~devaney/files/devaney11automatically.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">
Recorded music offers a wealth of information for studying performance practice. This paper examines the challenges of automatically extracting performance information from audio recordings of the singing voice and discusses our technique for automatically extracting information such as note timings, intonation, vibrato rates, and dynamics. An experiment is also presented that focuses on the tuning of semitones in solo soprano performances of Schubert’s &ldquo;Ave Maria&rdquo; by non-professional and professional singers. We found a small decrease in size of intervals with a leading tone function only in the non-professional group.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="mandel10a">6</a>]
</td>
<td class="bibtexitem">
Michael&nbsp;I. Mandel, Scott Bressler, Barbara Shinn-Cunningham, and Daniel P.&nbsp;W.
  Ellis.
 Evaluating source separation algorithms with reverberant speech.
 <em>IEEE Transactions on Audio, Speech, and Language Processing</em>,
  18(7):1872--1883, 2010.
[&nbsp;<a href="pubs_bib.html#mandel10a">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/TASL.2010.2052252">DOI</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/taslp10b.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">
This paper examines the performance of several source separation
systems on a speech separation task for which human intelligibility
has previously been measured.  For anechoic mixtures, automatic speech
recognition (ASR) performance on the separated signals is quite
similar to human performance.  In reverberation, however, while signal
separation has some benefit for ASR, the results are still far below
those of human listeners facing the same task.  Performing this same
experiment with a number of oracle masks created with <em>a priori</em>
knowledge of the separated sources motivates a new objective measure
of separation performance, the DERTM (Direct-path, Early echo, and
Reverberation, of the Target and Masker), which is closely related to
the ASR results.  This measure indicates that while the non-oracle
algorithms successfully reject the direct-path signal from the masking
source, they reject less of its reverberation, explaining the
disappointing ASR performance.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="mandel09a">7</a>]
</td>
<td class="bibtexitem">
Michael&nbsp;I. Mandel, Ron&nbsp;J. Weiss, and Daniel P.&nbsp;W. Ellis.
 Model-based expectation maximization source separation and
  localization.
 <em>IEEE Transactions on Audio, Speech, and Language Processing</em>,
  18(2):382--394, February 2010.
[&nbsp;<a href="pubs_bib.html#mandel09a">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/TASL.2009.2029711">DOI</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/taslp10.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">
This paper describes a system, referred to as model-based expectation-maximization source separation and localization (MESSL), for separating and localizing multiple sound sources from an underdetermined reverberant two-channel recording. By clustering individual spectrogram points based on their interaural phase and level differences, MESSL generates masks that can be used to isolate individual sound sources. We first describe a probabilistic model of interaural parameters that can be evaluated at individual spectrogram points. By creating a mixture of these models over sources and delays, the multi-source localization problem is reduced to a collection of single source problems. We derive an expectation-maximization algorithm for computing the maximum-likelihood parameters of this mixture model, and show that these parameters correspond well with interaural parameters measured in isolation. As a byproduct of fitting this mixture model, the algorithm creates probabilistic spectrogram masks that can be used for source separation. In simulated anechoic and reverberant environments, separations using MESSL produced on average a signal-to-distortion ratio 1.6 dB greater and perceptual evaluation of speech quality (PESQ) results 0.27 mean opinion score units greater than four comparable algorithms.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="mandel08b">8</a>]
</td>
<td class="bibtexitem">
Michael&nbsp;I. Mandel and Daniel P.&nbsp;W. Ellis.
 A web-based game for collecting music metadata.
 <em>Journal of New Music Research</em>, 37(2):151--165, 2008.
[&nbsp;<a href="pubs_bib.html#mandel08b">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1080/09298210802479300">DOI</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/jnmr08.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">
We have designed a web-based game, MajorMiner, that makes collecting descriptions of musical excerpts fun, easy, useful, and objective. Participants describe 10 second clips of songs and score points when their descriptions match those of other participants. The rules were designed to encourage players to be thorough and the clip length was chosen to make judgments objective and specific. To analyse the data, we measured the degree to which binary classifiers could be trained to spot popular tags. We also compared the performance of clip classifiers trained with MajorMiner's tag data to those trained with social tag data from a popular website. On the top 25 tags from each source, MajorMiner's tags were classified correctly 67.2% of the time, while the social tags were classified correctly 62.6% of the time.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="huang08">9</a>]
</td>
<td class="bibtexitem">
Thomas&nbsp;S. Huang, Charlie&nbsp;K. Dagli, Shyamsundar Rajaram, Edward&nbsp;Y. Chang,
  Michael&nbsp;I. Mandel, Graham&nbsp;E. Poliner, and Daniel P.&nbsp;W. Ellis.
 Active learning for interactive multimedia retrieval.
 <em>Proceedings of the IEEE</em>, 96(4):648--667, 2008.
[&nbsp;<a href="pubs_bib.html#huang08">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/JPROC.2008.916364">DOI</a>&nbsp;]
<blockquote><font size="-1">
As the first decade of the 21st century comes to a close, growth in multimedia delivery infrastructure and public demand for applications built on this backbone are converging like never before. The push towards reaching truly interactive multimedia technologies becomes stronger as our media consumption paradigms continue to change. In this paper, we profile a technology leading the way in this revolution: active learning. Active learning is a strategy that helps alleviate challenges inherent in multimedia information retrieval through user interaction. We show how active learning is ideally suited for the multimedia information retrieval problem by giving an overview of the paradigm and component technologies used with special attention given to the application scenarios in which these technologies are useful. Finally, we give insight into the future of this growing field and how it fits into the larger context of multimedia information retrieval.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="mandel06b">10</a>]
</td>
<td class="bibtexitem">
Michael&nbsp;I. Mandel, Graham&nbsp;E. Poliner, and Daniel P.&nbsp;W. Ellis.
 Support vector machine active learning for music retrieval.
 <em>Multimedia systems</em>, 12(1):1--11, August 2006.
[&nbsp;<a href="pubs_bib.html#mandel06b">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/s00530-006-0032-2">DOI</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/mmsj05.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">
Searching and organizing growing digital music collections requires a computational model of music similarity. This paper describes a system for performing flexible music similarity queries using SVM active learning. We evaluated the success of our system by classifying 1210 pop songs according to mood and style (from an online music guide) and by the performing artist. In comparing a number of representations for songs, we found the statistics of mel-frequency cepstral coefficients to perform best in precision-at-20 comparisons. We also show that by choosing training examples intelligently, active learning requires half as many labeled examples to achieve the same accuracy as a standard scheme.
</font></blockquote>
<p>
</td>
</tr>
</table>

<h2>Conference</h2>
<table>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="MaitiAndMandel2019b">1</a>]
</td>
<td class="bibtexitem">
Soumi Maiti and Michael&nbsp;I Mandel.
 Parametric resynthesis with neural vocoders.
 In <em>IEEE Workshop on Applications of Signal Processing to Audio
  and Acoustics</em>, 2019.
 To appear. Online: https://arxiv.org/abs/1906.06762.
[&nbsp;<a href="pubs_bib.html#MaitiAndMandel2019b">bib</a>&nbsp;| 
<a href="http://mr-pc.org/work/waspaa19/">Demo</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="MaitiAndMandel2019">2</a>]
</td>
<td class="bibtexitem">
Soumi Maiti and Michael&nbsp;I Mandel.
 Speech denoising by parametric resynthesis.
 In <em>Proceedings of the IEEE International Conference on
  Acoustics, Speech, and Signal Processing</em>, pages 6995--6999, 2019.
[&nbsp;<a href="pubs_bib.html#MaitiAndMandel2019">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/ICASSP.2019.8683130">DOI</a>&nbsp;| 
<a href="http://mr-pc.org/work/icassp19/">Demo</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/icassp19poster.pdf">Poster</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/icassp19.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">
This work proposes the use of clean speech vocoder parameters as the target for a neural network performing speech enhancement. These parameters have been designed for text-to-speech synthesis so that they both produce high-quality resyntheses and also are straightforward to model with neural networks, but have not been utilized in speech enhancement until now.  In comparison to a matched text-to-speech system that is given the ground truth transcripts of the noisy speech, our model is able to produce more natural speech because it has access to the true prosody in the noisy speech. In comparison to two denoising systems, the oracle Wiener mask and a DNN-based mask predictor, our model equals the oracle Wiener mask in subjective quality and intelligibility and surpasses the realistic system.  A vocoder-based upper bound shows that there is still room for improvement with this approach beyond the oracle Wiener mask. We test speaker-dependence with two speakers and show that a single model can be used for multiple speakers.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="TrinhEtAl2018">3</a>]
</td>
<td class="bibtexitem">
Viet&nbsp;Anh Trinh, Brian McFee, and Michael&nbsp;I Mandel.
 Bubble cooperative networks for identifying important speech cues.
 In <em>Proceedings of Interspeech</em>, pages 1616--1620, 2018.
[&nbsp;<a href="pubs_bib.html#TrinhEtAl2018">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.21437/Interspeech.2018-2377">DOI</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/interspeech18trinhPoster.pdf">Poster</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/interspeech18trinh.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">
Predicting the intelligibility of noisy recordings is difficult and most current algorithms treat all speech energy as equally important to intelligibility. Our previous work on human perception used a listening test paradigm and correlational analysis to show that some energy is more important to intelligibility than other energy. In this paper, we propose a system called the Bubble Cooperative Network (BCN), which aims to predict important areas of individual utterances directly from clean speech. Given such a prediction, noise is added to the utterance in unimportant regions and then presented to a recognizer.  The  BCN is trained with a loss that encourages it to add as much noise as possible while preserving recognition performance, encouraging it to identify important regions precisely and place the noise everywhere else.  Empirical evaluation shows that the BCN can obscure 97.7% of the spectrogram with noise while maintaining recognition accuracy for a simple speech recognizer that compares a noisy test utterance with a clean reference utterance.  The masks predicted by a single BCN on several utterances show patterns that are similar to analyses derived from human listening tests that analyze each utterance separately, while exhibiting better generalization and less context-dependence than previous approaches.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="SyedEtAl2018">4</a>]
</td>
<td class="bibtexitem">
Ali&nbsp;Raza Syed, Viet&nbsp;Anh Trinh, and Michael&nbsp;I. Mandel.
 Concatenative resynthesis with improved training signals for speech
  enhancement.
 In <em>Proceedings of Interspeech</em>, pages 1195--1199, 2018.
[&nbsp;<a href="pubs_bib.html#SyedEtAl2018">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.21437/Interspeech.2018-2439">DOI</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/interspeech18syedPoster.pdf">Poster</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/interspeech18syed.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">
Noise reduction in speech signals remains an important area of research with potential for high impact in speech processing domains such as voice communication and hearing prostheses.  We extend and demonstrate significant improvements to our previous work in synthesis-based speech enhancement, which performs concatenative resynthesis of speech signals for the production of noiseless, high quality speech.  Concatenative resynthesis methods perform unit selection through learned non-linear similarity functions between short chunks of clean and noisy signals.  These mappings are learned using deep neural networks (DNN) trained to predict high similarity for the exact chunk of speech that is contained within a chunk of noisy speech, and low similarity for all other pairings.  We find here that more robust mappings can be learned with a more efficient use of the available data by selecting pairings that are not exact matches, but contain similar clean speech that matches the original in terms of acoustic, phonetic, and prosodic content.  The resulting output is evaluated on the small vocabulary CHiME2-GRID corpus and outperforms our original baseline system in terms of intelligibility by combining phonetic similarity with similarity of acoustic intensity, fundamental frequency, and periodicity.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="MaitiEtAl2018">5</a>]
</td>
<td class="bibtexitem">
Soumi Maiti, Joey Ching, and Michael&nbsp;I. Mandel.
 Large vocabulary concatenative resynthesis.
 In <em>Proceedings of Interspeech</em>, pages 1190--1194, 2018.
[&nbsp;<a href="pubs_bib.html#MaitiEtAl2018">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.21437/Interspeech.2018-2383">DOI</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/interspeech18maitiPoster.pdf">Poster</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/interspeech18maiti.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">
Traditional speech enhancement systems reduce noise by modifying the noisy signal, which suffer from two problems: under-suppression of noise and over-suppression of speech. As an alternative, in this paper, we use the recently introduced concatenative resynthesis approach where we replace the noisy speech with its clean resynthesis. The output of such a system can produce speech that is both noise-free and high quality. This paper generalizes our previous small-vocabulary system to large vocabulary.  To do so, we employ efficient decoding techniques using fast approximate nearest neighbor (ANN) algorithms. Firstly, we  apply ANN techniques on the original small vocabulary task and get 5&#215; speedup.  We then apply the techniques to the construction of a large vocabulary concatenative resynthesis system and scale the system up to 12 &#215; larger dictionary. We perform listening tests with five participants to measure subjective quality and intelligibility of the output speech. 
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="MaitiAndMandel2017">6</a>]
</td>
<td class="bibtexitem">
Soumi Maiti and Michael&nbsp;I Mandel.
 Concatenative resynthesis using twin networks.
 In <em>Proceedings of Interspeech</em>, pages 3647--3651, 2017.
[&nbsp;<a href="pubs_bib.html#MaitiAndMandel2017">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.21437/Interspeech.2017-1653">DOI</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/interspeech17.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">
Traditional noise reduction systems modify a noisy signal to make it more like the original clean signal. For speech, these methods suffer from two main problems: under-suppression of noise and over-suppression of target speech. Instead, synthesizing clean speech based on the noisy signal could produce outputs that are both noise-free and high quality.  Our previous work introduced such a system using concatenative synthesis, but it required processing the clean speech at run time, which was slow and not scalable. In order to make such a system scalable, we propose here learning a similarity metric using two separate networks, one network processing the clean segments offline and another processing the noisy segments at run time. This system incorporates a ranking loss to optimize for the retrieval of appropriate clean speech segments. This model is compared against our original on the CHiME2-GRID corpus, measuring ranking performance and subjective listening tests of resyntheses.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="syed17">7</a>]
</td>
<td class="bibtexitem">
Ali Syed, Andrew Rosenberg, and Michael&nbsp;I Mandel.
 Active learning for low-resource speech recognition: Impact of
  selection size and language modeling data.
 In <em>Proceedings of the IEEE International Conference on
  Acoustics, Speech, and Signal Processing</em>, 2017.
[&nbsp;<a href="pubs_bib.html#syed17">bib</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/syed17.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">
Active learning aims to reduce the time and cost of developing speech recognition systems by selecting for transcription highly informative subsets from large pools of audio data.  Previous evaluations at OpenKWS and IARPA BABEL have investigated data selection for low-resource languages in very constrained scenarios with 2-hour data selections given a 1-hour seed set.  We expand on this to investigate what happens with larger selections and fewer constraints on language modeling data.  Our results, on four languages from the final BABEL OP3 period, show that active learning is helpful at larger selections with consistent gains up to 14 hours. We also find that the impact of additional language model data is orthogonal to the impact of the active learning selection criteria.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="devaney17">8</a>]
</td>
<td class="bibtexitem">
Johanna Devaney and Michael&nbsp;I Mandel.
 An evaluation of score-informed methods for estimating fundamental
  frequency and power from polyphonic audio.
 In <em>Proceedings of the IEEE International Conference on
  Acoustics, Speech, and Signal Processing</em>, 2017.
[&nbsp;<a href="pubs_bib.html#devaney17">bib</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/devaney17.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">
Robust extraction of performance data from polyphonic musical performances requires precise frame-level estimation of fundamental frequency (f0) and power. This paper evaluates a new score-guided approach to f0 and power estimation in polyphonic audio and compares the use of four different input features: the central bin frequencies of the spectrogram, the instantaneous frequency, and two variants of a high resolution spectral analysis. These four features were evaluated on four-part multi-track ensemble recordings, consisting of either four vocalists or bassoon, clarinet, saxophone, and violin (the Bach10 data set) created from polyphonic mixes of the monophonic tracks both with and without artificial reverberation. Score information was used to identify time-frequency regions of interest in the polyphonic mixes for each note in a corresponding aligned score, from which f0 and power estimates were made. The approach was able to recover ground truth f0 within 20 cents on average in reverberation and power within 5dB for anechoic mixtures, but only within 10dB for reverberant.

</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="mandel16b">9</a>]
</td>
<td class="bibtexitem">
Michael&nbsp;I Mandel and Jon&nbsp;P Barker.
 Multichannel spatial clustering for robust far-field automatic speech
  recognition in mismatched conditions.
 In <em>Proceedings of Interspeech</em>, pages 1991--1995, 2016.
[&nbsp;<a href="pubs_bib.html#mandel16b">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.21437/Interspeech.2016-1275">DOI</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/interspeech16bslides.pdf">Slides</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/interspeech16b.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">
Recent automatic speech recognition (ASR) results are quite good when the training data is matched to the test data, but much worse when they differ in some important regard, like the number and arrangement of microphones or differences in reverberation and noise conditions.  This paper proposes an unsupervised spatial clustering approach to microphone array processing that can overcome such train-test mismatches.  This approach, known as Model-based EM Source Separation and Localization (MESSL), clusters spectrogram points based on the relative differences in phase and level between pairs of microphones.  Here it is used for the first time to drive minimum variance distortionless response (MVDR) beamforming in several ways.  We compare it to a standard delay-and-sum beamformer on the CHiME-3 noisy test set (real recordings), using each system as a pre-processor for the same recognizer trained on the AMI meeting corpus. We find that the spatial clustering front end reduces word error rates by between 9.9 and 17.1% relative to the baseline.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="mandel16">10</a>]
</td>
<td class="bibtexitem">
Michael&nbsp;I Mandel.
 Directly comparing the listening strategies of humans and machines.
 In <em>Proceedings of Interspeech</em>, pages 660--664, 2016.
[&nbsp;<a href="pubs_bib.html#mandel16">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.21437/Interspeech.2016-932">DOI</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/interspeech16poster.pdf">Poster</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/interspeech16.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">
In a given noisy environment, human listeners can more accurately identify spoken words than automatic speech recognizers.  It is not clear, however, what information the humans are able to utilize in doing so that the machines are not.  This paper uses a recently introduced technique to directly characterize the information used by humans and machines on the same task.  The task was a forced choice between eight sentences spoken by a single talker from the small-vocabulary GRID corpus that were selected to be maximally confusable with one another.  These sentences were mixed with &ldquo;bubble&rdquo; noise, which is designed to reveal randomly selected time-frequency glimpses of the sentence.  Responses to these noisy mixtures allowed the identification of time-frequency regions that were important for each listener to recognize each sentence, i.e., regions that were frequently audible when a sentence was correctly identified and inaudible when it was not.  In comparing these regions across human and machine listeners, we found that dips in noise allowed the humans to recognize words based on informative speech cues.  In contrast, the baseline CHiME-2-GRID recognizer correctly identified sentences only when the time-frequency profile of the noisy mixture matched that of the underlying speech.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="erdogan16">11</a>]
</td>
<td class="bibtexitem">
Hakan Erdogan, John Hershey, Shinji Watanabe, Michael&nbsp;I Mandel, and Jonathan&nbsp;Le
  Roux.
 Improved MVDR beamforming using single-channel mask prediction
  networks.
 In <em>Proceedings of Interspeech</em>, pages 1981--1985, 2016.
[&nbsp;<a href="pubs_bib.html#erdogan16">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.21437/Interspeech.2016-552">DOI</a>&nbsp;| 
<a href="http://www.isca-speech.org/archive/Interspeech_2016/pdfs/0552.PDF">.PDF</a>&nbsp;]
<blockquote><font size="-1">
Recent studies on multi-microphone speech databases indicate
that it is beneficial to perform beamforming to improve speech
recognition accuracies, especially when there is a high level of
background noise. Minimum variance distortionless response
(MVDR) beamforming is an important beamforming method
that performs quite well for speech recognition purposes especially
if the steering vector is known. However, steering the
beamformer to focus on speech in unknown acoustic conditions
remains a challenging problem. In this study, we use single-channel
speech enhancement deep networks to form masks that
can be used for noise spatial covariance estimation, which steers
the MVDR beamforming toward the speech. We analyze how
mask prediction affects performance and also discuss various
ways to use masks to obtain the speech and noise spatial covariance
estimates in a reliable way. We show that using a
single mask across microphones for covariance prediction with
minima-limited post-masking yields the best result in terms of
signal-level quality measures and speech recognition word error
rates in a mismatched training condition.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="xiao16">12</a>]
</td>
<td class="bibtexitem">
Xiong Xiao, Shinji Watanabe, Hakan Erdogan, Liang Lu, John Hershey, Michael&nbsp;L
  Seltzer, Guoguo Chen, Yu&nbsp;Zhang, Michael Mandel, and Dong Yu.
 Deep beamforming networks for multi-channel speech recognition.
 In <em>Proceedings of the IEEE International Conference on
  Acoustics, Speech, and Signal Processing</em>, pages 5745--5749. IEEE, mar 2016.
[&nbsp;<a href="pubs_bib.html#xiao16">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/ICASSP.2016.7472778">DOI</a>&nbsp;| 
<a href="http://www.clsp.jhu.edu/~guoguo/papers/icassp2016_deep_beamforming.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">
Despite the significant progress in speech recognition enabled by deep neural networks, poor performance persists in some scenarios. In this work, we focus on far-field speech recognition which remains challenging due to high levels of noise and reverberation in the captured speech signals. We propose to represent the stages of acoustic processing including beamforming, feature extraction, and acoustic modeling, as three components of a single unified computational network. The parameters of a frequency-domain beamformer are first estimated by a network based on features derived from the microphone channels. These filter coefficients are then applied to the array signals to form an enhanced signal. Conventional features are then extracted from this signal and passed to a second network that performs acoustic modeling for classification. The parameters of both the beamforming and acoustic modeling networks are trained jointly using back-propagation with a common crossentropy objective function. In experiments on the AMI meeting corpus, we observed improvements by pre-training each sub-network with a network-specific objective function before joint training of both networks. The proposed method obtained a 3.2% absolute word error rate reduction compared to a conventional pipeline of independent processing stages.
</font></blockquote>
<p><blockquote><font size="-1">
Keywords: AMI meeting corpus,Acoustics,Array signal processing,Feature extraction,Microphones,Neural networks,Speech,Speech recognition,absolute word error rate reduction,acoustic modeling network,array signal application,array signal processing,backpropagation,beamforming,common crossentropy objective function,deep beamforming network,deep neural network,deep neural networks,direction of arrival,entropy,feature extraction,filter coefficient,filter- and-sum beamforming,filtering theory,frequency-domain analysis,frequency-domain beamformer,learning (artificial intelligence),microphone arrays,microphone channels,multichannel far-field speech recognition,network-specific objective function,reverberation,signal classification,signal enhancement,signal representation,single unified computational network,speech recognition
</font></blockquote>

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="bagchi15">13</a>]
</td>
<td class="bibtexitem">
Deblin Bagchi, Michael&nbsp;I Mandel, Zhongqiu Wang, Yanzhang He, Andrew Plummer,
  and Eric Fosler-Lussier.
 Combining spectral feature mapping and multi-channel model-based
  source separation for noise-robust automatic speech recognition.
 In <em>Proceedings of the IEEE Workshop on Automatic Speech
  Recognition and Understanding</em>, pages 496--503, 2015.
[&nbsp;<a href="pubs_bib.html#bagchi15">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/ASRU.2015.7404836">DOI</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/asru15.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">
Automatic Speech Recognition systems suffer from severe performance degradation in the presence of myriad complicating factors such as noise, reverberation, multiple speech sources, multiple recording devices, etc.  Previous challenges have sparked much innovation when it comes to designing systems capable of handling these complications.  In this spirit, the CHiME-3 challenge presents system builders with the task of recognizing speech in a real-world noisy setting wherein speakers talk to an array of 6 microphones in a tablet.  In order to address these issues, we explore the effectiveness of first applying a model-based source separation mask  to the output of a beamformer that combines the source signals recorded by each microphone, followed by a DNN-based front end spectral mapper that predicts clean filterbank features.  The source separation algorithm MESSL (Model-based EM Source Separation and Localization) has been extended from two channels to multiple channels in order to meet the demands of the challenge.  We report on interactions between the two systems, cross-cut by the use of a robust beamforming algorithm called BeamformIt.   Evaluations of different system settings reveal that combining MESSL and the spectral mapper together on the baseline beamformer algorithm boosts the performance substantially.

</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="tirumala15">14</a>]
</td>
<td class="bibtexitem">
Sreyas&nbsp;Srimath Tirumala and Michael&nbsp;I Mandel.
 Exciting estimated clean spectra for speech resynthesis.
 In <em>IEEE Workshop on Applications of Signal Processing to Audio
  and Acoustics</em>, 2015.
[&nbsp;<a href="pubs_bib.html#tirumala15">bib</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/waspaa15bposter.pdf">Poster</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/waspaa15b.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">
Spectral masking techniques are prevalent for noise suppression but they damage speech in regions of the spectrum where both noise and speech are present. This paper instead utilizes a recently introduced analysis-by-synthesis technique to estimate the spectral envelope of the speech at all frequencies, and adds to it a model of the speech excitation necessary to fully resynthesize a clean speech signal. Such a resynthesis should have little noise and high quality compared to mask-based approaches. We compare several different excitation signals on the Aurora4 corpus, including those derived from the high quefrency components of the noisy mixture and from the combination of a noise robust pitch tracker and a voiced/unvoiced classifier. Preliminary subjective evaluations suggest that the speech synthesized using our approach has higher voice quality and noise suppression than spectral masking.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="mandel15d">15</a>]
</td>
<td class="bibtexitem">
Michael&nbsp;I Mandel and Young&nbsp;Suk Cho.
 Audio super-resolution using concatenative resynthesis.
 In <em>IEEE Workshop on Applications of Signal Processing to Audio
  and Acoustics</em>, 2015.
[&nbsp;<a href="pubs_bib.html#mandel15d">bib</a>&nbsp;| 
<a href="http://mr-pc.org/work/waspaa15/">Demo</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/waspaa15slides.pdf">Slides</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/waspaa15.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">
  This paper utilizes a recently introduced non-linear
  dictionary-based denoising system in another voice mapping task,
  that of transforming low-bandwidth, low-bitrate speech into
  high-bandwidth, high-quality speech.  The system uses a deep neural
  network as a learned non-linear comparison function to drive unit
  selection in a concatenative synthesizer based on clean recordings.
  This neural network is trained to predict
  whether a given clean audio segment from the dictionary could be
  transformed into a given segment of the degraded observation.
  Speaker-dependent
  experiments on the small-vocabulary CHiME2-GRID corpus show that
  this model is able to resynthesize high quality clean speech from
  degraded observations.  Preliminary listening tests show that the
  system is able to improve subjective speech quality evaluations by
  up to 50 percentage points, while a similar system based on
  non-negative matrix factorization and trained on the same data
  produces no significant improvement.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="mandel15c">16</a>]
</td>
<td class="bibtexitem">
Michael&nbsp;I Mandel and Nicoleta Roman.
 Enforcing consistency in spectral masks using markov random fields.
 In <em>Proceedings of EUSIPCO</em>, pages 2028--2032, 2015.
[&nbsp;<a href="pubs_bib.html#mandel15c">bib</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/eusipco15.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">
Localization-based multichannel source separation
                  algorithms typically operate by clustering or
                  classifying individual time-frequency points based
                  on their spatial characteristics, treating adjacent
                  points as independent observations.  The Model-based
                  EM Source Separation and Localization (MESSL)
                  algorithm is one such approach for binaural signals
                  that achieves additional robustness by enforcing
                  consistency across frequencies in interaural phase
                  differences.  This paper incorporates MESSL into a
                  Markov Random Field (MRF) framework in order to
                  enforce consistency in the assignment of neighboring
                  time-frequency units to sources.  Approximate
                  inference in the MRF is performed using loopy belief
                  propagation, and the same approach can be used to
                  smooth any probabilistic source separation mask. The
                  proposed MESSL-MRF algorithm is tested on binaural
                  mixtures of three sources in reverberant conditions
                  and shows significant improvements over the original
                  MESSL algorithm as measured by both
                  signal-to-distortion ratios as well as a speech
                  intelligibility predictor.  
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="mandel14c">17</a>]
</td>
<td class="bibtexitem">
Michael&nbsp;I Mandel, Young-Suk Cho, and Yuxuan Wang.
 Learning a concatenative resynthesis system for noise suppression.
 In <em>Proceedings of the IEEE GlobalSIP conference</em>, 2014.
[&nbsp;<a href="pubs_bib.html#mandel14c">bib</a>&nbsp;| 
<a href="http://mr-pc.org/work/globalsip14/">Demo</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/globalsip14poster.pdf">Poster</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/globalsip14.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">
This paper introduces a new approach to dictionary-based source
  separation employing a learned non-linear metric.  In contrast to
  existing parametric source separation systems, this model is able to
  utilize a rich dictionary of speech signals.  In contrast to
  previous dictionary-based source separation systems, the system can
  utilize perceptually relevant non-linear features of the noisy and
  clean audio.  This approach utilizes a deep neural network (DNN) to
  predict whether a noisy chunk of audio contains a given clean chunk.
  Speaker-dependent experiments on the CHiME2-GRID corpus show 
  that this model is able to accurately resynthesize clean speech from
  noisy observations.  Preliminary listening tests show that the
  system's output has much higher audio quality than existing parametric
  systems trained on the same data, achieving noise suppression levels
  close to those of the original clean speech.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="mandel14b">18</a>]
</td>
<td class="bibtexitem">
Michael&nbsp;I Mandel, Sarah&nbsp;E Yoho, and Eric&nbsp;W Healy.
 Generalizing time-frequency importance functions across noises,
  talkers, and phonemes.
 In <em>Proceedings of Interspeech</em>, 2014.
[&nbsp;<a href="pubs_bib.html#mandel14b">bib</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/interspeech14poster.pdf">Poster</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/interspeech14.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">
Listeners can reliably identify speech in noisy conditions, although it is generally not known what specific features of speech are used to do this.  We utilize a recently introduced data-driven framework to identify these features.  By analyzing listening-test results involving the same speech utterance mixed with many different noise instances, the framework is able to compute the importance of each time-frequency point in the utterance to its intelligibility.  This paper shows that a trained model resulting from this framework can generalize to new conditions, successfully predicting the intelligibility of novel mixtures. First, it can generalize to novel noise instances after being trained on mixtures involving the same speech utterance but different noises. Second, it can generalize to novel talkers after being trained on mixtures involving the same syllables produced by different talkers in different noises. Finally, it can generalize to novel phonemes, after being trained on mixtures involving different consonants produced by the same or different talkers in different noises. Aligning the clean utterances in time and then propagating this alignment to the features used in the intelligibility prediction improves this generalization performance further.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="mandel14a">19</a>]
</td>
<td class="bibtexitem">
Michael&nbsp;I Mandel and Arun Narayanan.
 Analysis-by-synthesis feature estimation for robust automatic speech
  recognition using spectral masks.
 In <em>Proceedings of the IEEE International Conference on
  Acoustics, Speech, and Signal Processing</em>, 2014.
[&nbsp;<a href="pubs_bib.html#mandel14a">bib</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/icassp14poster.pdf">Poster</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/icassp14a.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">
Spectral masking is a promising method for noise suppression in which regions of the spectrogram that are dominated by noise are attenuated while regions dominated by speech are preserved. It is not clear, however, how best to combine spectral masking with the non-linear processing necessary to compute automatic speech recognition features. We propose an analysis-by-synthesis approach to automatic speech recognition, which, given a spectral mask, poses the estimation of mel frequency cepstral coefficients (MFCCs) of the clean speech as an optimization problem. MFCCs are found that minimize a combination of the distance from the resynthesized clean power spectrum to the regions of the noisy spectrum selected by the mask and the negative log likelihood under an unmodified large vocabulary continuous speech recognizer. In evaluations on the Aurora4 noisy speech recognition task with both ideal and estimated masks, analysis-by-synthesis decreases both word error rates and distances to clean speech as compared to traditional approaches.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="nandi14">20</a>]
</td>
<td class="bibtexitem">
Arnab Nandi, Lilong Jiang, and Michael&nbsp;I Mandel.
 Gestural query specification.
 In <em>Proceedings of the International Conference on Very Large
  Data Bases</em>, volume&nbsp;7, 2014.
[&nbsp;<a href="pubs_bib.html#nandi14">bib</a>&nbsp;| 
<a href="https://speakerdeck.com/arnabdotorg/gestural-query-specification-querying-without-keyboards">Slides</a>&nbsp;| 
<a href="http://www.vldb.org/pvldb/vol7/p289-nandi.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">
Direct, ad-hoc interaction with databases has typically been per-
formed over console-oriented conversational interfaces using query
languages such as SQL. With the rise in popularity of gestural user
interfaces and computing devices that use gestures as their exclusive
modes of interaction, database query interfaces require a fundamen-
tal rethinking to work without keyboards. We present a novel query
specification system that allows the user to query databases using a
series of gestures. We present a novel gesture recognition system
that uses both the interaction and the state of the database to classify
gestural input into relational database queries. We conduct exhaus-
tive systems performance tests and user studies to demonstrate that
our system is not only performant and capable of interactive laten-
cies, but it is also more usable, faster to use and more intuitive than
existing systems.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="mandel13">21</a>]
</td>
<td class="bibtexitem">
Michael&nbsp;I. Mandel.
 Learning an intelligibility map of individual utterances.
 In <em>IEEE Workshop on Applications of Signal Processing to Audio
  and Acoustics</em>, 2013.
[&nbsp;<a href="pubs_bib.html#mandel13">bib</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/waspaa13.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">
Predicting the intelligibility of noisy recordings is difficult and most current algorithms only aim to be correct on average across many recordings.  This paper describes a listening test paradigm and associated analysis technique that can predict the intelligibility of a specific recording of a word in the presence of a specific noise instance.  The analysis learns a map of the importance of each point in the recording's spectrogram to the overall intelligibility of the word when glimpsed through &ldquo;bubbles&rdquo; in many noise instances.  By treating this as a classification problem, a linear classifier can be used to predict intelligibility and can be examined to determine the importance of spectral regions.  This approach was tested on recordings of vowels and consonants. The important regions identified by the model in these tests agreed with those identified by a standard, non-predictive statistical test of independence and with the acoustic phonetics literature.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="roman13">22</a>]
</td>
<td class="bibtexitem">
Nicoleta Roman and Micheal Mandel.
 Classification based binaural dereverberation.
 In <em>Proceedings of Interspeech</em>, 2013.
[&nbsp;<a href="pubs_bib.html#roman13">bib</a>&nbsp;]
<blockquote><font size="-1">
Reverberation has a detrimental effect on speech perception both in terms of quality as well as intelligibility, as late reflections smear temporal and spectral cues. The ideal binary mask, which is an established computational approach to sound separation, was recently extended to remove reverberation. Experiments with both normal hearing and hearing impaired listeners have shown significant intelligibility improvements for reverberant speech processed using such a priori binary masks. The dereverberation problem can thus be formulated as a classification problem, where the desired output is the ideal binary mask. The goal in this approach is to produce a mask that selects the time-frequency regions where the direct energy dominates the energy from the late reflections. In this study, a binaural dereverberation algorithm is proposed which utilizes the binaural cues of interaural time and level differences as features. The algorithm is tested in highly reverberant environments using both simulated and recorded room impulse responses. Evaluations show significant improvements over the unprocessed condition as measured by both a speech quality measure and a speech intelligibility predictor.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="devaney12b">23</a>]
</td>
<td class="bibtexitem">
Johanna Devaney, Michael&nbsp;I. Mandel, and Ichiro Fujinaga.
 A study of intonation in three-part singing using the automatic music
  performance analysis and comparison toolkit (AMPACT).
 In <em>Proceedings of the International Society for Music
  Information Retrieval conference</em>, 2012.
[&nbsp;<a href="pubs_bib.html#devaney12b">bib</a>&nbsp;| 
<a href="http://ismir2012.ismir.net/event/papers/511-ismir-2012.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">
This paper introduces the Automatic Music Performance Analysis and Comparison Toolkit (AMPACT), is a MATLAB toolkit for accurately aligning monophonic audio to MIDI scores as well as extracting and analyzing timing-, pitch-, and dynamics-related performance data from the aligned recordings. This paper also presents the results of an analysis performed with AMPACT on an experiment studying intonation in three-part singing. The experiment examines the interval size and drift in four ensembles’ performances of a short exercise by Benedetti, which was designed to highlight the conflict between Just Intonation tuning and pitch drift.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="devaney11">24</a>]
</td>
<td class="bibtexitem">
Johanna Devaney, Michael&nbsp;I. Mandel, and Ichiro Fujinaga.
 Characterizing singing voice fundamental frequency trajectories.
 In <em>IEEE Workshop on Applications of Signal Processing to Audio
  and Acoustics</em>, pages 73--76, October 2011.
[&nbsp;<a href="pubs_bib.html#devaney11">bib</a>&nbsp;| 
<a href="http://music.mcgill.ca/~devaney/files/devaney11waspaaPoster.pdf">Poster</a>&nbsp;| 
<a href="http://www.music.mcgill.ca/~devaney/files/devaney11waspaa.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">
This paper evaluates the utility of the Discrete Cosine Transform (DCT) for characterizing singing voice fundamental frequency (F0) trajectories. Specifically, it focuses on the use of the 1 st and 2nd DCT coefficients as approximations of slope and curvature. It also considers the impact of vocal vibrato on the DCT calculations, including the influence of segmentation on the consistency of the reported DCT coefficient values. These characterizations are useful for describing similarities in the evolution of the fundamental frequency in different notes. Such descriptors can be applied in the areas of performance analysis and singing synthesis.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="mandel10b">25</a>]
</td>
<td class="bibtexitem">
Michael&nbsp;I. Mandel, Douglas Eck, and Yoshua Bengio.
 Learning tags that vary within a song.
 In <em>Proceedings of the International Society for Music
  Information Retrieval conference</em>, pages 399--404, August 2010.
[&nbsp;<a href="pubs_bib.html#mandel10b">bib</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/ismir10slides.pdf">Slides</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/ismir10.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">
This paper examines the relationship between human generated tags
  describing different parts of the same song.  These tags were
  collected using Amazon's Mechanical Turk service.  We find that the
  agreement between different people's tags decreases as the distance
  between the parts of a song that they heard increases.  To model
  these tags and these relationships, we describe a conditional
  restricted Boltzmann machine.  Using this model to fill in tags that
  should probably be present given a context of other tags, we train
  automatic tag classifiers (autotaggers) that
  outperform those trained on the original data.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="bergstra10">26</a>]
</td>
<td class="bibtexitem">
James Bergstra, Michael&nbsp;I. Mandel, and Douglas Eck.
 Scalable genre and tag prediction with spectral covariance.
 In <em>Proceedings of the International Society for Music
  Information Retrieval conference</em>, pages 507--512, August 2010.
[&nbsp;<a href="pubs_bib.html#bergstra10">bib</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/ismir10b.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">
Cepstral analysis is effective in separating source from filter in vocal and monophonic [pitched] recordings, but is it a good general-purpose framework for working with music audio? We evaluate covariance in spectral features as an alternative to means and variances in cepstral features (particularly MFCCs) as summaries of frame-level features. We find that spectral covariance is more effective than mean, variance, and covariance statistics of MFCCs for genre and social tag prediction. Support for our model comes from strong and state-of-the-art performance on the GTZAN genre dataset, MajorMiner, and MagnaTagatune. Our classification strategy based on linear classifiers is easy to implement, exhibits very little sensitivity to hyper-parameters, trains quickly (even for web-scale datasets), is fast to apply, and offers competitive performance in genre and tag prediction.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="mandel09b">27</a>]
</td>
<td class="bibtexitem">
Michael&nbsp;I. Mandel and Daniel P.&nbsp;W. Ellis.
 The ideal interaural parameter mask: a bound on binaural separation
  systems.
 In <em>IEEE Workshop on Applications of Signal Processing to Audio
  and Acoustics</em>, pages 85--88, October 2009.
[&nbsp;<a href="pubs_bib.html#mandel09b">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/ASPAA.2009.5346506">DOI</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/waspaa09poster.pdf">Poster</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/waspaa09.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">
We introduce the Ideal Interaural Parameter Mask as an upper bound on the performance of mask-based source separation algorithms that are based on the differences between signals from two microphones or ears. With two additions to our Model-based EM Source Separation and Localization system, its performance approaches that of the IIPM upper bound to within 0.9 dB. These additions battle the effects of reverberation by absorbing reverberant energy and by forcing the ILD estimate to be larger than it might otherwise be. An oracle reliability measure was also added, in the hope that estimating parameters from more reliable regions of the spectrogram would improve separation, but it was not consistently useful.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="devaney09">28</a>]
</td>
<td class="bibtexitem">
Johanna Devaney, Michael&nbsp;I. Mandel, and Daniel P.&nbsp;W. Ellis.
 Improving MIDI-audio alignment with acoustic features.
 In <em>IEEE Workshop on Applications of Signal Processing to Audio
  and Acoustics</em>, pages 45--48, October 2009.
[&nbsp;<a href="pubs_bib.html#devaney09">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/ASPAA.2009.5346500">DOI</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/devaney_waspaa09.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">
This paper describes a technique to improve the accuracy of dynamic time warping-based MIDI-audio alignment. The technique implements a hidden Markov model that uses aperiodicity and power estimates from the signal as observations and the results of a dynamic time warping alignment as a prior. In addition to improving the overall alignment, this technique also identifies the transient and steady state sections of the note. This information is important for describing various aspects of a musical performance, including both pitch and rhythm.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="law09">29</a>]
</td>
<td class="bibtexitem">
Edith Law, Kris West, Michael&nbsp;I Mandel, Mert Bay, and J.&nbsp;Stephen Downie.
 Evaluation of algorithms using games: the case of music annotation.
 In <em>Proceedings of the International Society for Music
  Information Retrieval conference</em>, pages 387--392, October 2009.
[&nbsp;<a href="pubs_bib.html#law09">bib</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/ismir09.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">
Search by keyword is an extremely popular method for retrieving music. To support this, novel algorithms that automatically tag music are being developed. The conventional way to evaluate audio tagging algorithms is to compute measures of agreement between the output and the ground truth set. In this work, we introduce a new method for evaluating audio tagging algorithms on a large scale by collecting set-level judgments from players of a human computation game called TagATune. We present the design and preliminary results of an experiment comparing five algorithms using this new evaluation metric, and contrast the results with those obtained by applying several conventional agreement-based evaluation metrics.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="weiss08">30</a>]
</td>
<td class="bibtexitem">
Ron&nbsp;J. Weiss, Michael&nbsp;I. Mandel, and Daniel P.&nbsp;W. Ellis.
 Source separation based on binaural cues and source model
  constraints.
 In <em>Proceedings of Interspeech</em>, pages 419--422, September 2008.
[&nbsp;<a href="pubs_bib.html#weiss08">bib</a>&nbsp;| 
<a href="http://www.isca-speech.org/archive/interspeech_2008/i08_0419.html">Demo</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/interspeech08.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">
We describe a system for separating multiple sources from a two-channel recording based on interaural cues and known characteristics of the source signals. We combine a probabilistic model of the observed interaural level and phase differences with a prior model of the source statistics and derive an EM algorithm for finding the maximum likelihood parameters of the joint model. The system is able to separate more sound sources than there are observed channels. In simulated reverberant mixtures of three speakers the proposed algorithm gives a signal-to-noise ratio improvement of 2.1 dB over a baseline algorithm using only interaural cues.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="mandel08a">31</a>]
</td>
<td class="bibtexitem">
Michael&nbsp;I. Mandel and Daniel P.&nbsp;W. Ellis.
 Multiple-instance learning for music information retrieval.
 In <em>Proceedings of the International Society for Music
  Information Retrieval conference</em>, pages 577--582, September 2008.
[&nbsp;<a href="pubs_bib.html#mandel08a">bib</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/ismir08poster.pdf">Poster</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/ismir08.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">
Multiple-instance learning algorithms train classifiers from lightly supervised data, i.e. labeled collections of items, rather than labeled items.  We compare the multiple-instance learners mi-SVM and MILES on the task of classifying 10-second song clips.  These classifiers are trained on tags at the track, album, and artist levels, or granularities, that have been derived from tags at the clip granularity, allowing us to test the effectiveness of the learners at recovering the clip labeling in the training set and predicting the clip labeling for a held-out test set.  We find that mi-SVM is better than a control at the recovery task on training clips, with an average classification accuracy as high as 87% over 43 tags; on test clips, it is comparable to the control with an average classification accuracy of up to 68%.  MILES performed adequately on the recovery task, but poorly on the test clips.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="ellis08">32</a>]
</td>
<td class="bibtexitem">
Daniel P.&nbsp;W. Ellis, Courtenay&nbsp;V. Cotton, and Michael&nbsp;I. Mandel.
 Cross-correlation of beat-synchronous representations for music
  similarity.
 In <em>Proceedings of the IEEE International Conference on
  Acoustics, Speech, and Signal Processing</em>, pages 57--60, April 2008.
[&nbsp;<a href="pubs_bib.html#ellis08">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/ICASSP.2008.4517545">DOI</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/icassp08.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">
Systems to predict human judgments of music similarity directly from the audio have generally been based on the global statistics of spectral feature vectors i.e. collapsing any large-scale temporal structure in the data. Based on our work in identifying alternative ("cover") versions of pieces, we investigate using direct correlation of beat-synchronous representations of music audio to find segments that are similar not only in feature statistics, but in the relative positioning of those features in tempo-normalized time. Given a large enough search database, good matches by this metric should have very high perceived similarity to query items. We evaluate our system through a listening test in which subjects rated system-generated matches as similar or not similar, and compared results to a more conventional timbral and rhythmic similarity baseline, and to random selections.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="mandel07c">33</a>]
</td>
<td class="bibtexitem">
Michael&nbsp;I. Mandel and Daniel P.&nbsp;W. Ellis.
 EM localization and separation using interaural level and phase
  cues.
 In <em>IEEE Workshop on Applications of Signal Processing to Audio
  and Acoustics</em>, pages 275--278, October 2007.
[&nbsp;<a href="pubs_bib.html#mandel07c">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/10.1109/ASPAA.2007.4392987">DOI</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/waspaa07poster.pdf">Poster</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/waspaa07.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">
We describe a system for localizing and separating multiple sound sources from a reverberant two-channel recording. It consists of a probabilistic model of interaural level and phase differences and an EM algorithm for finding the maximum likelihood parameters of this model. By assigning points in the interaural spectrogram probabilistically to sources with the best-fitting parameters and then estimating the parameters of the sources from the points assigned to them, the system is able to separate and localize more sound sources than there are available channels. It is also able to estimate frequency-dependent level differences of sources in a mixture that correspond well to those measured in isolation. In experiments in simulated anechoic and reverberant environments, the proposed system improved the signal-to-noise ratio of target sources by 2.7 and 3.4dB more than two comparable algorithms on average.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="mandel07b">34</a>]
</td>
<td class="bibtexitem">
Michael&nbsp;I. Mandel and Daniel P.&nbsp;W. Ellis.
 A web-based game for collecting music metadata.
 In Simon Dixon, David Bainbridge, and Rainer Typke, editors, <em>
  Proceedings of the International Society for Music Information Retrieval
  conference</em>, pages 365--366, September 2007.
[&nbsp;<a href="pubs_bib.html#mandel07b">bib</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/ismir07poster.pdf">Poster</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/ismir07.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">
We have designed a web-based game to make collecting descriptions of musical excerpts fun, easy, useful, and objective.  Participants describe 10 second clips of songs and score points when their descriptions match those of other participants.  The rules were designed to encourage users to be thorough and the clip length was chosen to make judgments more objective and specific.  Analysis of preliminary data shows that we are able to collect objective and specific descriptions of clips and that players tend to agree with one another.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="mandel07a">35</a>]
</td>
<td class="bibtexitem">
Michael&nbsp;I. Mandel, Daniel P.&nbsp;W. Ellis, and Tony Jebara.
 An EM algorithm for localizing multiple sound sources in
  reverberant environments.
 In B.&nbsp;Sch&ouml;lkopf, J.&nbsp;Platt, and T.&nbsp;Hoffman, editors, <em>Advances
  in Neural Information Processing Systems</em>, pages 953--960. MIT Press,
  Cambridge, MA, 2007.
[&nbsp;<a href="pubs_bib.html#mandel07a">bib</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/nips06poster.pdf">Poster</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/nips06.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">
We present a method for localizing and separating sound sources in stereo recordings that is robust to reverberation and does not make any assumptions about the source statistics.  The method consists of a probabilistic model of binaural multi-source recordings and an expectation maximization algorithm for finding the maximum likelihood parameters of that model.  These parameters include distributions over delays and assignments of time-frequency regions to sources.  We evaluate this method against two comparable algorithms on simulations of simultaneous speech from two or three sources.  Our method outperforms the others in anechoic conditions and performs as well as the better of the two in the presence of reverberation.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="mandel05">36</a>]
</td>
<td class="bibtexitem">
Michael&nbsp;I. Mandel and Daniel P.&nbsp;W. Ellis.
 Song-level features and support vector machines for music
  classification.
 In Joshua&nbsp;D. Reiss and Geraint&nbsp;A. Wiggins, editors, <em>Proceedings
  of the International Society for Music Information Retrieval conference</em>,
  pages 594--599, September 2005.
[&nbsp;<a href="pubs_bib.html#mandel05">bib</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/ismir05poster.pdf">Poster</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/ismir05.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">
Searching and organizing growing digital music collections requires automatic classification of music.  This paper describes a new system, tested on the task of artist identification, that uses support vector machines to classify songs based on features calculated over their entire lengths.  Since support vector machines are exemplar-based classifiers, training on and classifying entire songs instead of short-time features makes intuitive sense.  On a dataset of 1200 pop songs performed by 18 artists, we show that this classifier outperforms similar classifiers that use only SVMs or song-level features.  We also show that the KL divergence between single Gaussians and Mahalanobis distance between MFCC statistics vectors perform comparably when classifiers are trained and tested on separate albums, but KL divergence outperforms Mahalanobis distance when trained and tested on songs from the same albums.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="sudderth05">37</a>]
</td>
<td class="bibtexitem">
Erik&nbsp;B. Sudderth, Michael&nbsp;I. Mandel, William&nbsp;T. Freeman, and Alan&nbsp;S. Willsky.
 Distributed occlusion reasoning for tracking with nonparametric
  belief propagation.
 In Lawrence&nbsp;K. Saul, Yair Weiss, and L&eacute;on Bottou, editors, <em>
  Advances in Neural Information Processing Systems</em>, pages 1369--1376. MIT
  Press, Cambridge, MA, 2005.
[&nbsp;<a href="pubs_bib.html#sudderth05">bib</a>&nbsp;| 
<a href="http://ssg.mit.edu/nbp/">Demo</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/nips04.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">
We describe a three­dimensional geometric hand model suitable for visual tracking applications. The kinematic constraints implied by the model's joints have a probabilistic structure which is well described by a graphical model. Inference in this model is complicated by the hand's many degrees of freedom, as well as multimodal likelihoods caused by ambiguous image measurements. We use nonparametric belief propagation (NBP) to develop a tracking algorithm which exploits the graph's structure to control complexity, while avoiding costly discretization. While kinematic constraints naturally have a local structure, self­ occlusions created by the imaging process lead to complex interpendencies in color and edge­based likelihood functions. However, we show that local structure may be recovered by introducing binary hidden variables describing the occlusion state of each pixel. We augment the NBP algorithm to infer these occlusion variables in a distributed fashion, and then analytically marginalize over them to produce hand position estimates which properly account for occlusion events. We provide simulations showing that NBP may be used to refine inaccurate model initializations, as well as track hand motion through extended image sequences. 
</font></blockquote>
<p>
</td>
</tr>
</table>

<h2>Other</h2>
<table>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="GroverEtAl2018">1</a>]
</td>
<td class="bibtexitem">
Vikas Grover, Michael&nbsp;I Mandel, Valerie Shafer, Yusra Syed, and Austin Twine.
 Understanding acoustic cues non-native speakers use for identifying
  english /v/-/w/ using bubble noise method.
 In <em>ASHA Convention</em>, 2018.
[&nbsp;<a href="pubs_bib.html#GroverEtAl2018">bib</a>&nbsp;| 
<a href="https://plan.core-apps.com/asha2018/event/b70a14de3fdfa5add67a659fc86a3ef5">http</a>&nbsp;]
<blockquote><font size="-1">
Hindi speakers of English perceive the English /v/-/w/ contrast less accurately than English speakers (Grover et al., 2016). The specific acoustic information misperceived in /v/-/w/ contrast remains unclear. This study, using a novel method of “bubble” noise (Mandel et al., 2016), identifies the acoustic cues for perception of /v/-/w/ contrast in English and Hindi speakers of English.<p>
Learner Outcome(s):
Describe the effects of first language phonology on second language phonology
Discuss a novel method (Bubble Noise) to identify specific acoustic cues
Explain the importance of targeted training studies for difficult non-native contrasts<p>
Keywords: Speech perception, Bubble Noise, Non-native speakers, Hindi, Acoustic cues

</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="GhalyEtA2017">2</a>]
</td>
<td class="bibtexitem">
Hussein Ghaly and Michael&nbsp;I Mandel.
 Analyzing human and machine performance in resolving ambiguous spoken
  sentences.
 In <em>1st Workshop on Speech-Centric Natural Language Processing
  (SCNLP)</em>, pages 18--26, 2017.
[&nbsp;<a href="pubs_bib.html#GhalyEtA2017">bib</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/scnlp17.pdf">.pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="choi17">3</a>]
</td>
<td class="bibtexitem">
Jiyoung Choi and Michael&nbsp;I Mandel.
 Perception of korean fricatives and affricates in 'bubble' noise by
  native and nonnative speakers.
 In <em>International Circle of Korean Linguistics</em>, 2017.
[&nbsp;<a href="pubs_bib.html#choi17">bib</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="mandel15b">4</a>]
</td>
<td class="bibtexitem">
Michael&nbsp;I Mandel and Nicoleta Roman.
 Integrating markov random fields and model-based expectation
  maximization source separation and localization.
 In <em>Acoustical Society of America Spring Meeting</em>, 2015.
[&nbsp;<a href="pubs_bib.html#mandel15b">bib</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/asa15slides.pdf">Slides</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="mandel15a">5</a>]
</td>
<td class="bibtexitem">
Michael&nbsp;I Mandel, Sarah&nbsp;E Yoho, and Eric&nbsp;W Healy.
 Listener consistency in identifying speech mixed with particular
  “bubble” noise instances.
 In <em>Acoustical Society of America Spring Meeting</em>, 2015.
[&nbsp;<a href="pubs_bib.html#mandel15a">bib</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/asa15poster.pdf">Poster</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="mandel14d">6</a>]
</td>
<td class="bibtexitem">
Michael&nbsp;I Mandel and Song&nbsp;Hui Chon.
 Using auditory bubbles to determine spectro-temporal cues of timbre.
 In <em>Cognitively Based Music Informatics Research (CogMIR)</em>, 2014.
[&nbsp;<a href="pubs_bib.html#mandel14d">bib</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/cogmir14slides.pdf">Slides</a>&nbsp;]
<blockquote><font size="-1">
Listeners can reliably identify speech in noisy conditions, but it is not well understood which specific features of speech they use to do this.  This talk presents a data-driven framework for identifying these features.  By analyzing listening-test results involving the same speech utterance mixed with many different "bubble" noise instances, the framework is able to compute the importance of each time-frequency point in the utterance to its intelligibility, which we call the time-frequency importance function.  We show that listeners are self-consistent in their ability to identify the word in individual mixtures and are also fairly consistent with other listeners, and that different listeners' time-frequency importance functions are similar for the same utterance.  In addition, a predictive model trained under this framework is able to generalize to new conditions, successfully predicting the intelligibility of mixtures involving novel noise instances, novel utterances of the same word from the same and different talkers, and even to some extent novel consonants.  If there is time, I will also discuss a preliminary experiment applying this framework to the determination of the time-frequency points in a musical note that are most important to listeners for recognizing its timbre.

</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="nandi13">7</a>]
</td>
<td class="bibtexitem">
Arnab Nandi and Michael&nbsp;I Mandel.
 The interactive join: Recognizing gestures for database queries.
 In <em>CHI Works-In-Progress</em>, 2013.
[&nbsp;<a href="pubs_bib.html#nandi13">bib</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/chiwip13poster.pdf">Poster</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/chiwip13.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">
Direct, ad-hoc interaction with databases has typically
been performed over console-oriented conversational
interfaces using query languages such as SQL. With the
rise in popularity of gestural user interfaces and
computing devices that use gestures as their exclusive
mode of interaction, database query interfaces require a
fundamental rethinking to work without keyboards. Unlike
domain-specific applications, the scope of possible actions
is significantly larger if not infinite. Thus, the recognition
of gestures and their consequent queries is a challenge.
We present a novel gesture recognition system that uses
both the interaction and the state of the database to
classify gestural input into relational database queries.
Preliminary results show that using this approach allows
for fast, efficient and interactive gesture-based querying
over relational databases.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="mandel11a">8</a>]
</td>
<td class="bibtexitem">
Michael Mandel, Razvan Pascanu, Hugo Larochelle, and Yoshua Bengio.
 Autotagging music with conditional restricted boltzmann machines.
 March 2011.
 Online: http://arxiv.org/abs/1103.2832.
[&nbsp;<a href="pubs_bib.html#mandel11a">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1103.2832">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1103.2832">http</a>&nbsp;]
<blockquote><font size="-1">
This paper describes two applications of conditional restricted Boltzmann
machines (CRBMs) to the task of autotagging music. The first consists of
training a CRBM to predict tags that a user would apply to a clip of a song
based on tags already applied by other users. By learning the relationships
between tags, this model is able to pre-process training data to significantly
improve the performance of a support vector machine (SVM) autotagging. The
second is the use of a discriminative RBM, a type of CRBM, to autotag music. By
simultaneously exploiting the relationships among tags and between tags and
audio-based features, this model is able to significantly outperform SVMs,
logistic regression, and multi-layer perceptrons. In order to be applied to
this problem, the discriminative RBM was generalized to the multi-label setting
and four different learning algorithms for it were evaluated, the first such
in-depth analysis of which we are aware.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="mandel06a">9</a>]
</td>
<td class="bibtexitem">
Michael&nbsp;I. Mandel and Daniel P.&nbsp;W. Ellis.
 A probability model for interaural phase difference.
 In <em>ISCA Workshop on Statistical and Perceptual Audio
  Processing SAPA</em>, pages 1--6, 2006.
[&nbsp;<a href="pubs_bib.html#mandel06a">bib</a>&nbsp;| 
<a href="http://www.isca-speech.org/archive/sapa_2006/sap6_001.html">Demo</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/sapa06slides.pdf">Slides</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/sapa06.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">
In this paper, we derive a probability model for interaural phase differences at individual spectrogram points. Such a model can combine observations across arbitrary time and frequency regions in a structured way and does not make any assumptions about the characteristics of the sound sources. In experiments with speech from twenty speakers in simulated reverberant environments, this probabilistic method predicted the correct interaural delay of a signal more accurately than generalized cross-correlation methods.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="sudderth04">10</a>]
</td>
<td class="bibtexitem">
Erik&nbsp;B. Sudderth, Michael&nbsp;I. Mandel, William&nbsp;T. Freeman, and Alan&nbsp;S. Willsky.
 Visual hand tracking using nonparametric belief propagation.
 In <em>Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition Workshops</em>, pages 189--197, 2004.
[&nbsp;<a href="pubs_bib.html#sudderth04">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/CVPR.2004.200">DOI</a>&nbsp;| 
<a href="http://ssg.mit.edu/nbp/">Demo</a>&nbsp;| 
<a href="http://m.mr-pc.org/work/gmbv04.pdf">.pdf</a>&nbsp;]
<blockquote><font size="-1">
This paper develops probabilistic methods for visual tracking of a three-dimensional geometric hand model from monocular image sequences. We consider a redundant representation in which each model component is described by its position and orientation in the world coordinate frame. A prior model is then defined which enforces the kinematic constraints implied by the model's joints. We show that this prior has a local structure, and is in fact a pairwise Markov random field. Furthermore, our redundant representation allows color and edge-based likelihood measures, such as the Chamfer distance, to be similarly decomposed in cases where there is no self-occlusion. Given this graphical model of hand kinematics, we may track the hand's motion using the recently proposed nonparametric belief propagation (NBP) algorithm. Like particle filters, NBP approximates the posterior distribution over hand configurations as a collection of samples. However, NBP uses the graphical structure to greatly reduce the dimensionality of these distributions, providing improved robustness. Several methods are used to improve NBP's computational efficiency, including a novel KD-tree based method for fast Chamfer distance evaluation. We provide simulations showing that NBP may be used to refine inaccurate model initializations, as well as track hand motion through extended image sequences.
</font></blockquote>
<p>
</td>
</tr>
</table>

